{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0\n",
      "Your system does not contain a GPU that could be used by Tensorflow!\n",
      "0    0.88\n",
      "1    0.12\n",
      "Name: 4, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20203.78it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 24536.16it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 9960.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (TFBertForSequenceClassification, BertTokenizer)\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "\n",
    "# Restrict TensorFlow to only allocate 4GBs of memory on the first GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(f\"The system contains '{len(gpus)}' Physical GPUs and '{len(logical_gpus)}' Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(f\"Your system does not contain a GPU that could be used by Tensorflow!\")\n",
    "\n",
    "#read data\n",
    "def load_preprocess_data(limit=None):\n",
    "    #load data\n",
    "    data = np.load(\"../data/data_v4.npy\", allow_pickle=True)\n",
    "    data = np.delete(data, (0), axis=0)\n",
    "    data = data[data[:,4] != None]  #removing None sentiments\n",
    "    # data[:,4] = np.where(data[:,4] == '5', '1', data[:,4]) #change class 5->4\n",
    "    # data[:,4] = np.where(data[:,4] == '1', '1', data[:,4]) #change class 1->2\n",
    "    data[:,4] = np.where(data[:,4] == '1', 1, data[:,4]) #change class 1->1\n",
    "    data[:,4] = np.where(data[:,4] == '2', 1, data[:,4]) #change class 2->1\n",
    "    data[:,4] = np.where(data[:,4] == '3', 0, data[:,4]) #change class 1->0\n",
    "    data[:,4] = np.where(data[:,4] == '4', 1, data[:,4]) #change class 4->1\n",
    "    data[:,4] = np.where(data[:,4] == '5', 1, data[:,4]) #change class 5->1\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    if limit != None:\n",
    "        data = data[0:limit]\n",
    "\n",
    "    #sentiment distribution\n",
    "    print(data[4].value_counts(normalize = True))\n",
    "\n",
    "    #text preprocesing\n",
    "    # convert text to lowercase\n",
    "    # data[7] = data[7].apply(lambda x: [s.lower() for s in x])\n",
    "    # remove numbers\n",
    "    # data[7] = data[7].apply(lambda x: [s.replace(\"[0-9]\", \" \") for s in x])\n",
    "    # remove whitespaces\n",
    "    # data[5] = data[5].apply(lambda x: [' '.join(s.split()) for s in x])\n",
    "\n",
    "    #train test split\n",
    "    # train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[7], data[4], test_size=0.2, random_state=0)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "#make test, train, val sets\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = load_preprocess_data(200)\n",
    "\n",
    "#load the bert pretrained moed and tokenizer\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#prepare input for tokenizer\n",
    "pad_token = 0\n",
    "pad_token_segment_id = 0\n",
    "max_length = 128\n",
    "\n",
    "def convert_to_input(reviews):\n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "\n",
    "    for x in tqdm(reviews, position=0, leave=True):\n",
    "        inputs = bert_tokenizer.encode_plus(x, add_special_tokens=True, max_length=max_length)\n",
    "\n",
    "        i, t = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        m = [1] * len(i)\n",
    "\n",
    "        padding_length = max_length - len(i)\n",
    "\n",
    "        i = i + ([pad_token] * padding_length)\n",
    "        m = m + ([0] * padding_length)\n",
    "        t = t + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        input_ids.append(i)\n",
    "        attention_masks.append(m)\n",
    "        token_type_ids.append(t)\n",
    "\n",
    "    return [np.asarray(input_ids),\n",
    "            np.asarray(attention_masks),\n",
    "            np.asarray(token_type_ids)]\n",
    "\n",
    "X_test_input=convert_to_input(X_test)\n",
    "X_train_input=convert_to_input(X_train)\n",
    "X_val_input=convert_to_input(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on dataset\n",
      "Epoch 1/2\n",
      "35/35 [==============================] - 1013s 29s/step - loss: 0.3895 - accuracy: 0.8712 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "35/35 [==============================] - 839s 24s/step - loss: 0.3160 - accuracy: 0.8750 - val_loss: 0.3347 - val_accuracy: 0.9000\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#transform data to tensorflow format\n",
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "  return {\"input_ids\": input_ids,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_input[0], X_train_input[1],X_train_input[2],y_train)).map(example_to_features).shuffle(100).batch(24).repeat(5)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val_input[0], X_val_input[1],X_val_input[2],y_val)).map(example_to_features).batch(24)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_input[0], X_test_input[1],X_test_input[2],y_test)).map(example_to_features).batch(24)\n",
    "\n",
    "#prepare the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "print(\"Fine-tuning BERT on dataset\")\n",
    "bert_history = bert_model.fit(train_ds, epochs=2, validation_data=val_ds)\n",
    "\n",
    "#results aftera a few epochs\n",
    "results_true = test_ds.unbatch()\n",
    "results_true = np.asarray([element[1].numpy() for element in results_true])\n",
    "print(results_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions:\n",
      "F1 score: 0.0\n",
      "Accuracy score: 0.9\n"
     ]
    }
   ],
   "source": [
    "#predictions for the test\n",
    "results = bert_model.predict(test_ds)\n",
    "print(\"Model predictions:\")\n",
    "# for i in range(0,15):\n",
    "#     print(f\"\\t {results[0+i*128]}\")\n",
    "\n",
    "results_predicted = np.argmax(results, axis=1)\n",
    "\n",
    "#F-1 sore\n",
    "print(f\"F1 score: {f1_score(results_true, results_predicted)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(results_true, results_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
